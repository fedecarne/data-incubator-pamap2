<!DOCTYPE html>
<html lang="en">
<head>
  <title>Bootstrap Example</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="bootstrap-3.3.5-dist/css/bootstrap.min.css">
<link rel="stylesheet" href="bootstrap-3.3.5-dist/css/bootstrap-theme.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <style>
       
    .container {
    margin-right: 50px;
    margin-left: 150px;
    }

    header {    
    padding: 30px;
    margin-left: 130px;
    margin-right: 520px;
    }

    body {
    font: 20px "Montserrat", sans-serif;
    line-height: 1.8;
    }

    footer {
    background-color: #2d2d30;
    font: 15px "Montserrat", sans-serif;
    color: #f5f5f5;
    padding: 50px;
    }
    
  </style>
</head>
<body>


<header> 
    <div class="well well-lg">
    <h1>The Data Incubator Challenge</h1>
    </div>
</header>

<div class="container">
  
  <div class="row content">
    
    <div class="col-sm-2">
        
        <div class="list-group">

            <a href="#Problem"class="list-group-item">HAR</a>
            <a href="#Data"class="list-group-item">Data</a>
            <a href="#Analysis"class="list-group-item">Analysis</a>
            <a href="#Future"class="list-group-item">Future work</a>
                                
        </div>
    </div>
       
    <div class="col-sm-10">
      
      <div class="well well-lg">
      
        <h2 id="Problem">Human Activity Recognition</h2>
        <p> Human activity recognition (HAR) is a fast-growing technology. Understanding and automatically recognizing common human activities in real-life settings is likely to become a key feature in any intelligent system. </p>
        
        <p>In sensor-based HAR data from multiple body-worn inertial sensors (such as accelerometers and gyroscopes) is used to detect and classify human activities.</p>
        
        <p>For the TDI project I propose to study ....</p>
        
        <p>To movitate this proposal, here I present a preliminar analysis of one particular HAR datatset. 
        
        While I am focusing on this one particular dataset, there are many available HAR dataset online to explore. </p>
      </div>
      
      <div class="well well-lg">
        <h2 id="Data">PAMAP2 Dataset</h2>
        <p> The  <a href="https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring">PAMAP2 dataset</a> was collected at the Department of Augmented Vision of the German Research Center for Artificial Intelligence. It contains over 10 hours of recordings of 18 different physical activities (such as walking, cycling or running), performed by 9 subjects wearing 3 inertial measurement units.<p>
        <p> The sensors used for this dataset (<a href="http://media.wix.com/ugd/f221b8_f3ad435e76ac40448e7723c0ecab9cb8.pdf">Colibri wireless IMU)</a> were positioned in the wrist, chest and ankle. Each IMU comprised a 3D-accelerometer, a 3D-gyroscope sensor and a 3D-magnetometer.<p>
        <p> For more info about the dataset, check the following two publications:<p>
        <ul class="nav nav-pills nav-stacked">
          <li style="font-size:80%;"> [1] A. Reiss and D. Stricker. Introducing a New Benchmarked Dataset for Activity Monitoring. The 16th IEEE International Symposium on Wearable Computers (ISWC), 2012.</li>
          <li style="font-size:80%;">[2] A. Reiss and D. Stricker. Creating and Benchmarking a New Dataset for Physical Activity Monitoring. The 5th Workshop on Affect and Behaviour Related Assistance (ABRA), 2012.</li>
        </ul>
        
      </div>
      
      <div class="well well-lg">
        <h2 id="Analysis">Preliminary Analysis</h2>
        <p> For this preliminary analysis, I'll focus on two activities: running and cycling. The goal is to find <b>what's the best sensor (type and position) to automatically detect whether a user is running or cycling?</b> This is a practical question with direct application. It is required, for example, for any fitness tracker device. </p>
        
        <p>Let's first inspect how the measured signals look in the time domain. The plot below shows traces recorded by each sensor (accelerometer, gyroscope and magnetometer) attached to the wrist during running and cycling. Each plot shows three traces, corresponding to the three spatial axes (x,y,z). </p>
        
        <br>
        <center><figure>
        <img src="img/signals-1.png" class="img-rounded" alt="" width="800" height="500">
        <figcaption>Fig1. - Sensor traces for the wrist-attached IMU while running (top) and cycling (bottom).</figcaption>
        </figure></center> 
        <br>

        <p>As we can see, the recorded traces show specific temporal patterns that are quite different in each activity.</p>
        
        <p>Let's now see how the data looks like 'state-space'. The plot below shows, as an example, all datapoints for one of the gyroscope sensors. <p>
        
        
        <br>
        <center><figure>
        <img src="img/project-1.png" class="img-rounded" alt="" width="800" height="250">
        <figcaption>Fig2. - Sensor projections for the wrist-attached gyroscope in both activities.</figcaption>
        </figure></center> 
        <br>
        
        <p>We see that, while the data forms different clusters for each activity, they are clearly not linearly separable. </p>
        <p> Let's then use a SVM to find what sensor is the most useful to distinguish runing from cycling. </p> 
        <p> I've trained different SVM's to solve the classification problem using each sensor data separately. In the figure below, I plot the classification error for each sensor (accelerometer, gyroscope and magnetometer) in each body position (wrist, chest and ankle).</p>
        
        <br>
        <center><figure>        
        </p><img src="img/error-1.png" class="img-rounded" alt="" width="800" height="300" align="middle">
        <figcaption>Fig3. - Classification error for each sensor and body position.</figcaption>
        </figure></center> 
        <br>
        
        <p> The classification error was computed using a 10-fold approach.</p>
        
        <p> So we can conclude that the optimal choice to distinguish running versus cycling is to use an accelerometer in the wrist. This is a great, since that there's where sport watches are worn. So, we can use this method to automatically detect whether a user is running or cycling and correcly label the activity before saving it in our activity tracker database. <p>
      </div>
      
      <div class="well well-lg">
        <h2 id="Future">Future work</h2>
        <p> Human activity recognition is an extremely promising field. With the fast development of new wearable sensors, the amount of data is increasing exponentially. This grow needs to be match with new algorithms that can effectively decode information from this data. In this project, I intend to find out what are the best algorithms and the best sensors to classify human activity. This includes: ...distinguising multiple activities, ... Moreover, the solution has to be robust between different individuals so that ...
            </p>
      </div>
    </div>
  </div>
</div>

<footer class="container-fluid text-center">
    <h4>Federico J Carnevale</h4>
    <br>
    <ul class="list-inline">
<li><a href="mailto:fedecarne@gmail.com" class="icoEnvelope" title="Email"><i class="fa fa-envelope fa-3x"></i></a></li>
<li><a href="http://www.github.com/fedecarne" class="icoGithub" title="Facebook"><i class="fa fa-github fa-3x"></i></a></li>
<li><a href="https://www.linkedin.com/in/fedecarnevale" class="icoLinkedin" title="Linkedin"><i class="fa fa-linkedin fa-3x"></i></a></li>
</ul>
</footer>

</body>
</html>
