<!DOCTYPE html>
<html lang="en">
<head>
  <title>TDI Challenge</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="bootstrap-3.3.5-dist/css/bootstrap.min.css">
<link rel="stylesheet" href="bootstrap-3.3.5-dist/css/bootstrap-theme.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <style>
       
    .container {
    margin-right: 50px;
    margin-left: 150px;
    }

    header {    
    padding: 30px;
    margin-left: 130px;
    margin-right: 520px;
    }

    body {
    font: 20px "Montserrat", sans-serif;
    line-height: 1.8;
    }

    footer {
    background-color: #2d2d30;
    font: 15px "Montserrat", sans-serif;
    color: #f5f5f5;
    padding: 50px;
    }
    
  </style>
</head>
<body>


<header> 
    <div class="well well-lg">
    <h1>The Data Incubator Challenge</h1>
    </div>
</header>

<div class="container">
  
  <div class="row content">
    
    <div class="col-sm-2">
        
        <div class="list-group">

            <a href="#Problem"class="list-group-item">HAR</a>
            <a href="#Data"class="list-group-item">Data</a>
            <a href="#Analysis"class="list-group-item">Analysis</a>
            <a href="#Future"class="list-group-item">Future work</a>
                                
        </div>
    </div>
       
    <div class="col-sm-10">
      
    <div class="page-header">
        <h2 id="Problem">Human Activity Recognition</h2>
    </div>

    <div class="well well-lg">
      

        <p> Human activity recognition (HAR) is an exciting and fast-growing technology. Understanding and automatically recognizing common human activities in real-life settings is likely to become a key feature in future intelligent systems. 
        
        <p>Sensor-based HAR uses data from multiple body-worn inertial sensors, such as accelerometers, gyroscopes and magnetometers, to detect and classify human activities. This requires algorithms that can efficiently transform large high-dimensional data into activity labels. For the TDI project, I propose to study machine learning techniques to solve sensor-based HAR. I will focus on:
        <ul>
          <li> <b> What's the best ML approach to recognize human activities from sensor data?</b></li>
          <li> <b> What are the best sensors (type and body location) for this purpose?</b></li>
        </ul>
        <p> I am particularly interested in recognizing fitness activities based on smartphone sensor data. This would allow us to easily keep track of our fitness activity in daily life. </p>

<p>         The different approaches will be judged in terms of (1) <b>accuracy</b>: how many classification errors are produced (2) <b>efficiency</b>: how much data is needed and how fast is the convergence, and (3) <b>robustness</b>: how good is the generalization to different individuals. </p>
        <p>To motivate this proposal, I present a preliminary analysis of one HAR datatset. I focus on distinguishing two activities from three different sensors attatched to three different body parts. From this first analysis, I can conclude which is the optimal sensor and what is the best sensor location. </p>
        <p> Finally, while I am focusing on one particular example, there are many available HAR dataset online to explore, such as:
        <ul>
          <li>  <a href="http://www5.cs.fau.de/activitynet/benchmark-datasets/daliac-daily-life-activities/">The DaLiAc dataset (19 subjects, 13 activities)</a>  </li>
          <li>  <a href="http://sipi.usc.edu/HAD/">The USC-SIPI Human Activity Dataset (14 subjects, 12 activities)</a></li>
        </ul>
        </p>
      </div>
      
      
  <div class="page-header">
        <h2 id="Data">PAMAP2 Dataset</h2>
    </div>
    
    
      <div class="well well-lg">

        <p> The  <a href="https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring">PAMAP2 dataset</a> was collected at the Department of Augmented Vision of the German Research Center for Artificial Intelligence. It contains over 10 hours of recordings of 18 different physical activities (such as walking, cycling or running), performed by 9 subjects wearing 3 inertial measurement units.<p>
        <p> The sensors used for this dataset (<a href="http://media.wix.com/ugd/f221b8_f3ad435e76ac40448e7723c0ecab9cb8.pdf">Colibri wireless IMU)</a> were positioned in the wrist, chest and ankle. Each IMU comprised a 3D-accelerometer, a 3D-gyroscope sensor and a 3D-magnetometer.<p>
        <p> For more info about the dataset, check the following two publications:<p>
        <ul class="nav nav-pills nav-stacked">
          <li style="font-size:80%;"> [1] A. Reiss and D. Stricker. Introducing a New Benchmarked Dataset for Activity Monitoring. The 16th IEEE International Symposium on Wearable Computers (ISWC), 2012.</li>
          <li style="font-size:80%;">[2] A. Reiss and D. Stricker. Creating and Benchmarking a New Dataset for Physical Activity Monitoring. The 5th Workshop on Affect and Behaviour Related Assistance (ABRA), 2012.</li>
        </ul>
        
      </div>
      
        <div class="page-header">
        <h2 id="Analysis">Preliminary Analysis</h2>
    </div>
      
      <div class="well well-lg">

        <p> For this preliminary analysis, I'll focus on two activities: running and cycling. The goal is to find <b>what's the best sensor (type and position) to automatically detect whether a user is running or cycling?</b> This is a practical question with direct application. It is required, for example, for a fitness tracker device to correctly tag an activity in its database. </p>
        
        <p>Let's first inspect how the measured signals look like in the time domain. The plot below shows traces recorded by each sensor (accelerometer, gyroscope and magnetometer) attached to the wrist during running and cycling. Each plot shows three traces, corresponding to the three spatial axes (x,y,z). </p>
        
        <br>
        <center><figure>
        <img src="img/signals-1.png" class="img-rounded" alt="" width="800" height="500">
        <figcaption>Fig1. - Sensor traces for the wrist-attached IMU while running (top) and cycling (bottom).</figcaption>
        </figure></center> 
        <br>

        <p>As we can see, the recorded traces show specific temporal patterns that are quite different for each activity. Another way of looking at the data is in the 'state-space'. The following plot shows, as an example, all data points for one of the gyroscope sensors. <p>
        
        
        <br>
        <center><figure>
        <img src="img/project-1.png" class="img-rounded" alt="" width="800" height="250">
        <figcaption>Fig2. - Sensor projections for the wrist-attached gyroscope in both activities.</figcaption>
        </figure></center> 
        <br>
        
        <p>We see that while the data is structured in different regions for each activity, they are clearly not linearly separable. Therefore, I decided to use a nonlinear SVM with a Gaussian kernel to find which sensor is the most useful to distinguish running from cycling. </p> 
        <p> I've trained 9 different SVM's to solve the classification problem using each sensor data separately. Below, I plot the classification error for each sensor (accelerometer, gyroscope and magnetometer) in each body part (wrist, chest and ankle).</p>
        
        <br>
        <center><figure>        
        </p><img src="img/error-1.png" class="img-rounded" alt="" width="800" height="300" align="middle">
        <figcaption>Fig3. - Classification error for each sensor and body part.</figcaption>
        </figure></center> 
        <br>
               
        <p> As we can see, the optimal choice to distinguish running versus cycling is to use an accelerometer on the wrist. This is great news, since that's where sport watches are worn :) </p>
        <p> That means that this method can be used to automatically detect whether a user is running or cycling and correctly label the activity when saving it to our activity tracker database. <p>
      </div>
      
        <div class="page-header">
        <h2 id="Future">Future work</h2>
    </div>              
              
      <div class="well well-lg">

        <p> Human activity recognition is an extremely promising field. Combined with wearable devices, the amount of available data is likely to keep increasing exponentially. This data growth must be matched with new algorithms that can effectively decode and extract information from this high-dimensional data. <b>In this project, I intend to find out how ML can solve HAR.</b>
        </p>
      </div>
    </div>
  </div>
</div>

<footer class="container-fluid text-center">
    <h4>Federico J Carnevale</h4>
    <br>
    <ul class="list-inline">
<li><a href="mailto:fedecarne@gmail.com" class="icoEnvelope" title="Email"><i class="fa fa-envelope fa-3x"></i></a></li>
<li><a href="http://www.github.com/fedecarne" class="icoGithub" title="Facebook"><i class="fa fa-github fa-3x"></i></a></li>
<li><a href="https://www.linkedin.com/in/fedecarnevale"><i class="fa fa-linkedin fa-3x"></i></a></li>
</ul>
</footer>

</body>
</html>
